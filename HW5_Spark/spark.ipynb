{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4db18d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=sparkApp>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('sparkApp').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "print(sc)\n",
    "numeric_val = sc.parallelize([1,2,3,4])\n",
    "numeric_val.map(lambda x:x*x*x).collect()\n",
    "sc.stop\n",
    "import mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed3ef9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import module\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "#create session\n",
    "appName = \"Recommender system in Spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7853c8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+\n",
      "|UserId|MovieId|Label|\n",
      "+------+-------+-----+\n",
      "|     1|     11|    5|\n",
      "|     1|     85|    3|\n",
      "|     1|    249|    4|\n",
      "|     1|    260|    5|\n",
      "|     1|    265|    4|\n",
      "+------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------+---------+\n",
      "|UserId|MovieId|trueLabel|\n",
      "+------+-------+---------+\n",
      "|     1|     36|        5|\n",
      "|     1|    150|        5|\n",
      "|     1|    261|        4|\n",
      "|     1|    356|        5|\n",
      "|     1|    531|        5|\n",
      "+------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "number of training data rows: 719625 , number of testing data rows: 180248\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data_orin = spark.read.csv('./data/movieRating.csv', inferSchema=True, header=True).select(\"UserId\", \"MovieId\", \"Rating\")\n",
    "# split training set and test set\n",
    "splits = data_orin.randomSplit([0.8, 0.2], seed=10)\n",
    "train = splits[0].withColumnRenamed(\"Rating\", \"Label\")\n",
    "test = splits[1].withColumnRenamed(\"Rating\", \"trueLabel\")\n",
    "# aaa.printSchema()\n",
    "train.show(5)\n",
    "test.show(5)\n",
    "train_rows = train.count()\n",
    "test_rows = test.count()\n",
    "print (\"number of training data rows:\", train_rows, \n",
    "       \", number of testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8481b8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "# define ALS (Alternating Least Square) as recommendation model\n",
    "als = ALS(maxIter=19, regParam=0.01, userCol=\"UserId\", \n",
    "          itemCol=\"MovieId\", ratingCol=\"Label\")\n",
    "#train ALS model\n",
    "model = als.fit(train)\n",
    "print(\"Training is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afa04886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing is done!\n"
     ]
    }
   ],
   "source": [
    "# generate prediction\n",
    "prediction = model.transform(test)\n",
    "print(\"testing is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea224f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: nan\n"
     ]
    }
   ],
   "source": [
    "#import RegressionEvaluator to calculate MAE\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(prediction)\n",
    "print (\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edb91ab5",
   "metadata": {},
   "source": [
    "prediction.count()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad3c617c",
   "metadata": {},
   "source": [
    "a = 180744\n",
    "cleanPred = prediction.dropna(how=\"any\", subset=[\"prediction\"])\n",
    "b = cleanPred.count()\n",
    "print(\"number of rows after dropping data with missing value: \", b)\n",
    "print(\"number of missing data: \", a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1a2a14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of original data rows:  180248\n",
      "number of rows after dropping data with missing value:  180226\n",
      "number of missing data:  22\n"
     ]
    }
   ],
   "source": [
    "# 因為MAE=nan，判斷原本的prediction中有空值，需刪除\n",
    "a = prediction.count()\n",
    "print(\"number of original data rows: \", a)\n",
    "#drop rows with any missing data\n",
    "cleanPred = prediction.dropna(how=\"any\", subset=[\"prediction\"])\n",
    "b = cleanPred.count()\n",
    "print(\"number of rows after dropping data with missing value: \", b)\n",
    "print(\"number of missing data: \", a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc971d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae: 0.6972219239848941\n"
     ]
    }
   ],
   "source": [
    "# generate MAE\n",
    "mae = evaluator.evaluate(cleanPred)\n",
    "print (\"mae:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cec942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write prediction result into csv file\n",
    "pred = prediction.toPandas()\n",
    "pred.to_csv('./result/prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab4f67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root Mean Square Error (RMSE): 0.9012930533111924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26a8758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't calculate regression model's accuracy !!!!!!!! "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6038c23c",
   "metadata": {},
   "source": [
    "type(prediction)\n",
    "prediction.printSchema()\n",
    "prediction.select('prediction').show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "362d909a",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "pre = np.array(cleanPred.select(\"prediction\").collect())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7873778",
   "metadata": {},
   "source": [
    "max(cleanPred.select(\"prediction\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a953a763",
   "metadata": {},
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c50f93d3",
   "metadata": {},
   "source": [
    "cleanTest = test.dropna(how=\"any\", subset=[\"trueLabel\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6eeb5266",
   "metadata": {},
   "source": [
    "cleanTest.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "241f33ad",
   "metadata": {},
   "source": [
    "tes = np.array(cleanTest.select(\"trueLabel\").collect())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce4092ed",
   "metadata": {},
   "source": [
    "len(pre)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98521abe",
   "metadata": {},
   "source": [
    "len(tes)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0260a1b1",
   "metadata": {},
   "source": [
    "a = pd.read_csv('./data/movieRating.csv')\n",
    "a\n",
    "min(a['UserID'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b226361",
   "metadata": {},
   "source": [
    "max(pre)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ac7b1b9",
   "metadata": {},
   "source": [
    "a.info()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb1104f8",
   "metadata": {},
   "source": [
    "data = spark.read.csv(\"./data/movieRating.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "data.first()\n",
    "# type(data)\n",
    "df = data.selectExpr(\"Rating as rrr\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d3f2fbf",
   "metadata": {},
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "rates_data = Rating.map(lambda x: Rating(int(x[0]),int(x[1]),int(x[2])))\n",
    "rates_data.first()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a7eec7e",
   "metadata": {},
   "source": [
    "from  pyspark.mllib.recommendation import ALS\n",
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "sc.setCheckpointDir('checkpoint/')\n",
    "ALS.checkpointInterval = 2\n",
    "model = ALS.train(ratings=rates_data, rank=20, iterations=5, lambda_=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02929c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
