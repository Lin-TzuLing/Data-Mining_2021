{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "151320f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics \n",
    "from sklearn.feature_extraction import text\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303b8af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   business_id  10000 non-null  object\n",
      " 1   date         10000 non-null  object\n",
      " 2   review_id    10000 non-null  object\n",
      " 3   stars        10000 non-null  int64 \n",
      " 4   text         10000 non-null  object\n",
      " 5   type         10000 non-null  object\n",
      " 6   user_id      10000 non-null  object\n",
      " 7   cool         10000 non-null  int64 \n",
      " 8   useful       10000 non-null  int64 \n",
      " 9   funny        10000 non-null  int64 \n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "yelp = pd.read_csv('./data/yelp.csv')\n",
    "yelp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab35f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  My wife took me here on my birthday for breakf...      5\n",
       "1  I have no idea why some people give bad review...      5\n",
       "2  love the gyro plate. Rice is so good and I als...      4\n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5\n",
       "4  General Manager Scott Petello is a good egg!!!...      5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 讀取csv檔僅保留\"text\"、\"stars\"兩個欄位\n",
    "data = yelp[['text','stars']]\n",
    "display(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4c1342e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my wife took me here on my birthday for breakf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have no idea why some people give bad review...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love the gyro plate. rice is so good and i als...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rosie, dakota, and i love chaparral dog park!!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>general manager scott petello is a good egg!!!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>first visit...had lunch here today - used my g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>should be called house of deliciousness!\\n\\ni ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>i recently visited olive and ivy for business ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>my nephew just moved to scottsdale recently so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4-5 locations.. all 4.5 star average.. i think...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars\n",
       "0     my wife took me here on my birthday for breakf...      1\n",
       "1     i have no idea why some people give bad review...      1\n",
       "2     love the gyro plate. rice is so good and i als...      1\n",
       "3     rosie, dakota, and i love chaparral dog park!!...      1\n",
       "4     general manager scott petello is a good egg!!!...      1\n",
       "...                                                 ...    ...\n",
       "9995  first visit...had lunch here today - used my g...      0\n",
       "9996  should be called house of deliciousness!\\n\\ni ...      1\n",
       "9997  i recently visited olive and ivy for business ...      1\n",
       "9998  my nephew just moved to scottsdale recently so...      0\n",
       "9999  4-5 locations.. all 4.5 star average.. i think...      1\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 將stars欄位內值大於等於4的轉成1，其餘轉成0\n",
    "data.loc[data['stars']<4 , 'stars'] = 0 \n",
    "data.loc[data['stars']>=4 , 'stars'] = 1 \n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "\n",
    "display(data)\n",
    "# 1: positive, 0: negative"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c8a9217",
   "metadata": {},
   "source": [
    "d1 = 'a b d e d f a f e fa d s a b n'\n",
    "d2 = 'a z a f e fa h'\n",
    "d3 = 'a z a f e fa h'\n",
    "vectorizer = CountVectorizer(stop_words='english', token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")  \n",
    "X = vectorizer.fit_transform([d1,d2,d3])\n",
    "r = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "print(\"CountVector\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21c9a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves',\n",
    "        'he','him','his','himself','she','her','hers','herself,it','its','itself','they','them','their,theirs',\n",
    "        'themselves','what','which','who','whom','this','that','these','those','am','is','are','was','were',\n",
    "        'be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but',\n",
    "        'if','or','because','as','until','while','of','at','by','for','with','about','against','between',\n",
    "        'into','through','during','before','after','above','below','to','from','up','down','in','out','on',\n",
    "        'off','over','under','again','further','then','once','here','there','when','where','why','how','all',\n",
    "        'any','both','each','few','more','most','other','some','such','no','nor','not','only','own','same',\n",
    "        'so','than','too','very','s','t','can','will','just','don','should','now','\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbb4072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('./data/stop_word.pickle', 'rb') as data:\n",
    "#     delete_col = pickle.load(data)\n",
    "len(text.ENGLISH_STOP_WORDS.union(stop_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632c27c",
   "metadata": {},
   "source": [
    "len(set1)\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(stop_list)\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union('My')\n",
    "list(text.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ef5a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>text_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wife took birthday breakfast excellent. weathe...</td>\n",
       "      <td>1</td>\n",
       "      <td>[wife, took, birthday, breakfast, excellent, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>idea people bad reviews place. goes you, every...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idea, people, bad, reviews, place, goes, you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love gyro plate. rice good dig candy selection :)</td>\n",
       "      <td>1</td>\n",
       "      <td>[love, gyro, plate, rice, good, dig, candy, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rosie, dakota, love chaparral dog park!!! it's...</td>\n",
       "      <td>1</td>\n",
       "      <td>[rosie, dakota, love, chaparral, dog, convenie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>general manager scott petello good egg!!! deta...</td>\n",
       "      <td>1</td>\n",
       "      <td>[general, manager, scott, petello, good, detai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>visit...had lunch today - groupon. ordered bru...</td>\n",
       "      <td>0</td>\n",
       "      <td>[visit, had, lunch, today, groupon, ordered, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>called house deliciousness! item, item, blah b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[called, house, item, item, blah, blah, blah, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>recently visited olive ivy business week, 3 vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[recently, visited, olive, ivy, business, week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>nephew moved scottsdale recently bunch friends...</td>\n",
       "      <td>0</td>\n",
       "      <td>[nephew, moved, scottsdale, recently, bunch, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4-5 locations.. 4.5 star average.. think arizo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[locations, star, average, think, arizona, fan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars  \\\n",
       "0     wife took birthday breakfast excellent. weathe...      1   \n",
       "1     idea people bad reviews place. goes you, every...      1   \n",
       "2     love gyro plate. rice good dig candy selection :)      1   \n",
       "3     rosie, dakota, love chaparral dog park!!! it's...      1   \n",
       "4     general manager scott petello good egg!!! deta...      1   \n",
       "...                                                 ...    ...   \n",
       "9995  visit...had lunch today - groupon. ordered bru...      0   \n",
       "9996  called house deliciousness! item, item, blah b...      1   \n",
       "9997  recently visited olive ivy business week, 3 vi...      1   \n",
       "9998  nephew moved scottsdale recently bunch friends...      0   \n",
       "9999  4-5 locations.. 4.5 star average.. think arizo...      1   \n",
       "\n",
       "                                             text_split  \n",
       "0     [wife, took, birthday, breakfast, excellent, w...  \n",
       "1     [idea, people, bad, reviews, place, goes, you,...  \n",
       "2     [love, gyro, plate, rice, good, dig, candy, se...  \n",
       "3     [rosie, dakota, love, chaparral, dog, convenie...  \n",
       "4     [general, manager, scott, petello, good, detai...  \n",
       "...                                                 ...  \n",
       "9995  [visit, had, lunch, today, groupon, ordered, b...  \n",
       "9996  [called, house, item, item, blah, blah, blah, ...  \n",
       "9997  [recently, visited, olive, ivy, business, week...  \n",
       "9998  [nephew, moved, scottsdale, recently, bunch, f...  \n",
       "9999  [locations, star, average, think, arizona, fan...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a_test = [['My','Lazy','Dog'],['My', 'Happy', 'Cat']]\n",
    "# a_test = [['however my ,My Lazy Dog'],['My Happy Cat']]\n",
    "data['text_split'] = ''\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "count=0\n",
    "for i in range(len(data)):\n",
    "    # 去除停頓詞stop words\n",
    "    data.text.iloc[i] = remove_stopwords(data.text.iloc[i])\n",
    "    \n",
    "    # 將text欄位內的文字利用分割符號切割\n",
    "    split = re.split(';|,|\\s|,\\s|\\.|\\*|\\n',data.iloc[i]['text'])\n",
    "    split_new = [word for word in split if word.isalpha()]\n",
    "    data.text_split.iloc[i] = split_new\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfdc5758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26797"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1 = {'ini'}\n",
    "for i in range(len(data)):\n",
    "    list1 = data.iloc[i]['text_split']\n",
    "    for item in list1:\n",
    "        set1.add(item)\n",
    "set1.remove('ini')\n",
    "len(set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ecedfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>text_split</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wife took birthday breakfast excellent. weathe...</td>\n",
       "      <td>1</td>\n",
       "      <td>[wife, took, birthday, breakfast, excellent, w...</td>\n",
       "      <td>wife took birthday breakfast excellent weather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>idea people bad reviews place. goes you, every...</td>\n",
       "      <td>1</td>\n",
       "      <td>[idea, people, bad, reviews, place, goes, you,...</td>\n",
       "      <td>idea people bad reviews place goes you everyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love gyro plate. rice good dig candy selection :)</td>\n",
       "      <td>1</td>\n",
       "      <td>[love, gyro, plate, rice, good, dig, candy, se...</td>\n",
       "      <td>love gyro plate rice good dig candy selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rosie, dakota, love chaparral dog park!!! it's...</td>\n",
       "      <td>1</td>\n",
       "      <td>[rosie, dakota, love, chaparral, dog, convenie...</td>\n",
       "      <td>rosie dakota love chaparral dog convenient sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>general manager scott petello good egg!!! deta...</td>\n",
       "      <td>1</td>\n",
       "      <td>[general, manager, scott, petello, good, detai...</td>\n",
       "      <td>general manager scott petello good detail let ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>visit...had lunch today - groupon. ordered bru...</td>\n",
       "      <td>0</td>\n",
       "      <td>[visit, had, lunch, today, groupon, ordered, b...</td>\n",
       "      <td>visit had lunch today groupon ordered bruschet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>called house deliciousness! item, item, blah b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[called, house, item, item, blah, blah, blah, ...</td>\n",
       "      <td>called house item item blah blah blah dont waz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>recently visited olive ivy business week, 3 vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[recently, visited, olive, ivy, business, week...</td>\n",
       "      <td>recently visited olive ivy business week visit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>nephew moved scottsdale recently bunch friends...</td>\n",
       "      <td>0</td>\n",
       "      <td>[nephew, moved, scottsdale, recently, bunch, f...</td>\n",
       "      <td>nephew moved scottsdale recently bunch friends...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4-5 locations.. 4.5 star average.. think arizo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[locations, star, average, think, arizona, fan...</td>\n",
       "      <td>locations star average think arizona fantastic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars  \\\n",
       "0     wife took birthday breakfast excellent. weathe...      1   \n",
       "1     idea people bad reviews place. goes you, every...      1   \n",
       "2     love gyro plate. rice good dig candy selection :)      1   \n",
       "3     rosie, dakota, love chaparral dog park!!! it's...      1   \n",
       "4     general manager scott petello good egg!!! deta...      1   \n",
       "...                                                 ...    ...   \n",
       "9995  visit...had lunch today - groupon. ordered bru...      0   \n",
       "9996  called house deliciousness! item, item, blah b...      1   \n",
       "9997  recently visited olive ivy business week, 3 vi...      1   \n",
       "9998  nephew moved scottsdale recently bunch friends...      0   \n",
       "9999  4-5 locations.. 4.5 star average.. think arizo...      1   \n",
       "\n",
       "                                             text_split  \\\n",
       "0     [wife, took, birthday, breakfast, excellent, w...   \n",
       "1     [idea, people, bad, reviews, place, goes, you,...   \n",
       "2     [love, gyro, plate, rice, good, dig, candy, se...   \n",
       "3     [rosie, dakota, love, chaparral, dog, convenie...   \n",
       "4     [general, manager, scott, petello, good, detai...   \n",
       "...                                                 ...   \n",
       "9995  [visit, had, lunch, today, groupon, ordered, b...   \n",
       "9996  [called, house, item, item, blah, blah, blah, ...   \n",
       "9997  [recently, visited, olive, ivy, business, week...   \n",
       "9998  [nephew, moved, scottsdale, recently, bunch, f...   \n",
       "9999  [locations, star, average, think, arizona, fan...   \n",
       "\n",
       "                                              processed  \n",
       "0     wife took birthday breakfast excellent weather...  \n",
       "1     idea people bad reviews place goes you everyon...  \n",
       "2         love gyro plate rice good dig candy selection  \n",
       "3     rosie dakota love chaparral dog convenient sur...  \n",
       "4     general manager scott petello good detail let ...  \n",
       "...                                                 ...  \n",
       "9995  visit had lunch today groupon ordered bruschet...  \n",
       "9996  called house item item blah blah blah dont waz...  \n",
       "9997  recently visited olive ivy business week visit...  \n",
       "9998  nephew moved scottsdale recently bunch friends...  \n",
       "9999  locations star average think arizona fantastic...  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words = [row[2] for row in data.itertuples(index=False, name=None)]\n",
    "data['processed'] = data['text_split'].apply(lambda x: \" \".join(x) )\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95105ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(data)):\n",
    "    aaa = data.text.iloc[i]\n",
    "    bbb = data.processed.iloc[i]\n",
    "    if aaa==bbb:\n",
    "        count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "589e0d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaaalright</th>\n",
       "      <th>aaaamazing</th>\n",
       "      <th>aaand</th>\n",
       "      <th>aah</th>\n",
       "      <th>aand</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aarp</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>zupa</th>\n",
       "      <th>zupas</th>\n",
       "      <th>zur</th>\n",
       "      <th>zuzu</th>\n",
       "      <th>zuzus</th>\n",
       "      <th>zweigel</th>\n",
       "      <th>zzzzzzzzzzzzzzzzz</th>\n",
       "      <th>éclairs</th>\n",
       "      <th>école</th>\n",
       "      <th>ém</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 26498 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aaa  aaaaaalright  aaaamazing  aaand  aah  aand  aaron  aarp  ab  \\\n",
       "0      0    0             0           0      0    0     0      0     0   0   \n",
       "1      0    0             0           0      0    0     0      0     0   0   \n",
       "2      0    0             0           0      0    0     0      0     0   0   \n",
       "3      0    0             0           0      0    0     0      0     0   0   \n",
       "4      0    0             0           0      0    0     0      0     0   0   \n",
       "...   ..  ...           ...         ...    ...  ...   ...    ...   ...  ..   \n",
       "9995   0    0             0           0      0    0     0      0     0   0   \n",
       "9996   0    0             0           0      0    0     0      0     0   0   \n",
       "9997   0    0             0           0      0    0     0      0     0   0   \n",
       "9998   0    0             0           0      0    0     0      0     0   0   \n",
       "9999   0    0             0           0      0    0     0      0     0   0   \n",
       "\n",
       "      ...  zupa  zupas  zur  zuzu  zuzus  zweigel  zzzzzzzzzzzzzzzzz  éclairs  \\\n",
       "0     ...     0      0    0     0      0        0                  0        0   \n",
       "1     ...     0      0    0     0      0        0                  0        0   \n",
       "2     ...     0      0    0     0      0        0                  0        0   \n",
       "3     ...     0      0    0     0      0        0                  0        0   \n",
       "4     ...     0      0    0     0      0        0                  0        0   \n",
       "...   ...   ...    ...  ...   ...    ...      ...                ...      ...   \n",
       "9995  ...     0      0    0     0      0        0                  0        0   \n",
       "9996  ...     0      0    0     0      0        0                  0        0   \n",
       "9997  ...     0      0    0     0      0        0                  0        0   \n",
       "9998  ...     0      0    0     0      0        0                  0        0   \n",
       "9999  ...     0      0    0     0      0        0                  0        0   \n",
       "\n",
       "      école  ém  \n",
       "0         0   0  \n",
       "1         0   0  \n",
       "2         0   0  \n",
       "3         0   0  \n",
       "4         0   0  \n",
       "...     ...  ..  \n",
       "9995      0   0  \n",
       "9996      0   0  \n",
       "9997      0   0  \n",
       "9998      0   0  \n",
       "9999      0   0  \n",
       "\n",
       "[10000 rows x 26498 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'vocabulary_S'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-47c31fd08a4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_S\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'vocabulary_S'"
     ]
    }
   ],
   "source": [
    "# 去除停頓詞stop words\n",
    "stop_word = text.ENGLISH_STOP_WORDS.union(stop_list)\n",
    "\n",
    "text_str = [row[3] for row in data.itertuples(index=False, name=None)]\n",
    "# text = [row[2] for row in data.itertuples(index=False, name=None)]\n",
    "\n",
    "# vec = CountVectorizer(stop_words='english' , analyzer=lambda x:x)\n",
    "vec = CountVectorizer(stop_words=stop_word, analyzer='word', lowercase=False)\n",
    "x = vec.fit_transform(text_str)\n",
    "\n",
    "term = pd.DataFrame(x.toarray(), columns=vec.get_feature_names())\n",
    "display(term)\n",
    "vocab = vec.vocabulary_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8818843",
   "metadata": {},
   "source": [
    "term.columns\n",
    "term_col = []\n",
    "delete_col = []\n",
    "for col in term.columns:\n",
    "    term_col.append(term[col].sum())\n",
    "    if term[col].sum()>5:\n",
    "        delete_col.append(col)\n",
    "len(delete_col)\n",
    "with open('./data/stop_word.pickle', 'wb') as f:\n",
    "    pickle.dump(delete_col, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# delete_col"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac5fc8c3",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "a = np.array(term_col)\n",
    "np.median(a)\n",
    "np.quantile(a,0.75)\n",
    "# np.max(a)\n",
    "# np.min(a)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8945c608",
   "metadata": {},
   "source": [
    "with open('./data/stop_word.pickle', 'rb') as f:\n",
    "    delete_col = pickle.load(f)\n",
    "delete_col"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed53d000",
   "metadata": {},
   "source": [
    "# stop_words1 = stop_words.union(delete_col)\n",
    "# del vec,x,term,set1\n",
    "# term.drop(stop_word1,axis=1)\n",
    "\n",
    "vec = CountVectorizer(stop_words=delete_col, analyzer='word', lowercase=False)\n",
    "x = vec.fit_transform(text_str)\n",
    "term = pd.DataFrame(x.toarray(), columns=vec.get_feature_names())\n",
    "#  analyzer='word', lowercase=False\n",
    "# display(term)\n",
    "vec.vocabulary_\n",
    "# term = term.drop(delete_col,axis=1)\n",
    "term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4306df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformer = TfidfTransformer(smooth_idf=True)\n",
    "Z = transformer.fit_transform(x)\n",
    "r = pd.DataFrame(Z.toarray(),columns=vec.get_feature_names())\n",
    "display(r)\n",
    "# r\n",
    "# r.columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb277ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "r['wife'].sum()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f295a3c6",
   "metadata": {},
   "source": [
    "https://ithelp.ithome.com.tw/articles/10228481"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b01050ba",
   "metadata": {},
   "source": [
    "# len(data.columns)\n",
    "# data.info()\n",
    "# len(term.columns)\n",
    "# len(term)\n",
    "term.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=350, max_depth=25)\n",
    "\n",
    "\n",
    "def k_fold(k, data, term):\n",
    "    size = int(len(data)/k)\n",
    "    acc = 0\n",
    "    for i in range(0,k):\n",
    "        # 訓練特徵         \n",
    "        train_word = pd.concat([term[:i*size],term[(i+1)*size:]])\n",
    "        # 訓練答案         \n",
    "        train_star = pd.concat([data[:i*size],data[(i+1)*size:]])['stars']\n",
    "        # 測試資料         \n",
    "        test_word = term[i*size:(i+1)*size]\n",
    "        # 測試答案        \n",
    "        test_star = data[i*size:(i+1)*size]['stars']\n",
    "        print('{}, {}'.format(len(train_word),len(test_word)))\n",
    "        #訓練模型         \n",
    "        model.fit(train_word, train_star)\n",
    "        predict_ans = model.predict(test_word)\n",
    "        acc += metrics.accuracy_score(test_star, predict_ans)\n",
    "    return acc/k\n",
    "        \n",
    "        \n",
    "print(len(data))\n",
    "# del vec\n",
    "# test_ans = [0,1]\n",
    "k_fold(4,data,r)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18b09ddd",
   "metadata": {},
   "source": [
    "0.6864000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa23dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "words = list(data.text_split)\n",
    "display(data.text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20549be",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vec.get_feature_names())\n",
    "# list(vocab)\n",
    "# if 'aa' in list(vocab):\n",
    "#     print('1')\n",
    "# vocab['aaaaaalright']\n",
    "vocab1 = dict(sorted(vocab.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(words, size=300,min_count=1)\n",
    "# min_count要設定成1，因為默認5，會過濾掉出現較少次數的詞\n",
    "\n",
    "word_embed1 = w2v_model[vocab1]\n",
    "\n",
    "import numpy as np\n",
    "# display(word_embed)\n",
    "np.shape(word_embed1)\n",
    "\n",
    "# w2v_model.wv.vocab\n",
    "# len(list(vocab))\n",
    "# np.shape(word_embed)\n",
    "\n",
    "# term = 10000*26498\n",
    "# word_vec = 26498*300(embed_size) \n",
    "\n",
    "# term排序跟不一樣\n",
    "w2v_matrix = np.matmul(term, word_embed1) \n",
    "w2v_matrix\n",
    "# np.shape(w2v_matrix)\n",
    "w2v_result = pd.DataFrame(w2v_matrix)\n",
    "w2v_result"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8c6436e",
   "metadata": {},
   "source": [
    "# model = Word2Vec(sentences=words,size=100, window=5, workers=4)\n",
    "# model.save(\"word2vec.model\")\n",
    "# model = Word2Vec.load(\"word2vec.model\")\n",
    "# model.train(words, epochs=1, total_words=20000)\n",
    "def most_similar(w2v_model, words, topn=10):\n",
    "    similar_df = pd.DataFrame()\n",
    "    for word in words:\n",
    "        try:\n",
    "            similar_words = pd.DataFrame(w2v_model.wv.most_similar(word, topn=topn), columns=[word, 'cos'])\n",
    "            similar_df = pd.concat([similar_df, similar_words], axis=1)\n",
    "        except:\n",
    "            print(word, \"not found in Word2Vec model!\")\n",
    "    return similar_df\n",
    "\n",
    "\n",
    "# model.wv.vocab\n",
    "# vector = model.wv['00am']  # get numpy vector of a word\n",
    "# sims = model.wv.most_similar('perfect', topn=10)\n",
    "# sims\n",
    "# word_vectors = model.wv\n",
    "# word_vectors"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c8973ed",
   "metadata": {},
   "source": [
    "# display(model.wv.vocab)\n",
    "most_similar(w2v_model,['breakfast'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a25dab2f",
   "metadata": {},
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "# Train a bigram detector.(multi-words)\n",
    "bigram_transformer = Phrases(text['processed'])\n",
    "\n",
    "# Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.\n",
    "model = Word2Vec(bigram_transformer[text['processed']], min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a140895",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = RandomForestClassifier(n_estimators=350, max_depth=25)\n",
    "\n",
    "\n",
    "def k_fold(k, data, term):\n",
    "    size = int(len(data)/k)\n",
    "    acc = 0\n",
    "    for i in range(0,k):\n",
    "        # 訓練特徵         \n",
    "        train_word = pd.concat([term[:i*size],term[(i+1)*size:]])\n",
    "        # 訓練答案         \n",
    "        train_star = pd.concat([data[:i*size],data[(i+1)*size:]])['stars']\n",
    "        # 測試資料         \n",
    "        test_word = term[i*size:(i+1)*size]\n",
    "        # 測試答案        \n",
    "        test_star = data[i*size:(i+1)*size]['stars']\n",
    "        print('{}, {}'.format(len(train_word),len(test_word)))\n",
    "        #訓練模型         \n",
    "        model.fit(train_word, train_star)\n",
    "        predict_ans = model.predict(test_word)\n",
    "        acc += metrics.accuracy_score(test_star, predict_ans)\n",
    "        print(metrics.accuracy_score(test_star, predict_ans))\n",
    "    return acc/k\n",
    "        \n",
    "        \n",
    "print(len(data))\n",
    "# del vec\n",
    "k_fold(4,data,w2v_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6b4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2928483a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
